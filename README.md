# Apache Spark
Этот гайд предназначен для тех, кто хочет развернуть Apache Spark. Все действия выполнялись на ОС Windows 10/11, в остальных ОС установку можно выполнять через командную строку, а не графический интерфейс согласно официальной [документации](https://spark.apache.org/).

Необходимый стек для выполнения процедур: 
* Java Development Kit ([скачать](https://www.oracle.com/cis/java/technologies/downloads/), я брал самую последнюю версию на момент написания гайда 22.0.1);
* Apache Spark ([скачать](https://spark.apache.org/), я брал самую последнюю версию на момент написания гайда 3.5.1);
* Python ([скачать](https://www.python.org/), можно взять самую новую версию проблем быть не должно - на момент написания гайда 3.12.3);
* Jupyter Notebook (проще всего получить, скачав [Anaconda](https://www.anaconda.com/));
* Hadoop и winutils.exe (см. ниже в гайде).

Перед установкой всех программ следует внимательно следить за их расположением на жестком диске, поскольку это потребуется для добавления директорий в `PATH`!!!

## 1. Установка Python, Java Development Kit (JDK) и Anaconda
Проще всего установить согласно инструкции, никаких проблем с установкой возникнуть не должно:
* в моем случае путь для Python - `C:\Users\Имя вашего пользователя\AppData\Local\Programs\Python\Python312\python.exe`;
* в моем случае путь для JDK - `C:\Program Files\Java\jdk-22`;
* путь для Anaconda на установку Spark не влияет, она нужна для запуска `Jupyter Notebook`.

## 2. Установка Apache Spark
Проще всего скачать архив с официального сайта (см. выше) и извлечь в какую-нибудь директорию Вашего компьютера, переименовать папку по необходимости (в моем случае: я переименовал название папки в `Apache Spark`, поэтому директория называется так: `C:\Program Files\Apache Spark`). Внутри директории множество папок и файлов - `bin`, `conf`, `data` и т.д.

## 3. Установка Apache Hadoop и winutils.exe
Из [репозитория](https://github.com/steveloughran/winutils/tree/master) скачать подходящую версию Hadoop (в нашем случае это папка `hadoop-3.0.0/bin`, он совместим со скачанной версией Spark). Внутри этой папки есть файлы `winutils.exe` и `hadoop.dll`, от этих файлов работоспособность Spark зависит в первую очередь. Папку следует извлечь в какую-нибудь директорию Вашего компьютера, переименовать папку по необходимости (в моем случае: я переименовал название папки в `Apache Hadoop`, поэтому директория называется так: `C:\Program Files\Apache Hadoop`). Внутри директории должна быть только папка `bin`.

Внимание: существуют также сторонние [репозитории](https://github.com/cdarlint/winutils) с другими, более новыми версиями Hadoop, при выборе которых следует стоит быть очень осторожным, поскольку если Вы возьмете не ту версию Hadoop для Spark, то могут возникнуть конфликты (об этом чуть ниже).

## 4. Добавление в PATH
Далее следует зайти в `Мой компьютер` -> `Свойства` -> `Дополнительно` -> `Переменные среды` - для разных версий Windows методология может немного отличаться (например, в ОС Windows 11 может выскочить окно, в котором нужно выбрать пункт `Дополнительные параметры системы`), но в целом принцип нахождения аналогичен. 

Далее следует создать 4 переменных среды в верхнем окне переменных среды (верхнее окно):
```python
HADOOP_HOME = путь до папки c Apache Hadoop (в моем случае - C:\Program Files\Apache Hadoop)
PYSPARK_HOME = путь до папки с Python (в моем случае - C:\Users\Имя вашего пользователя\AppData\Local\Programs\Python\Python312\python.exe)
JAVA_HOME = путь до папки с JDK (C:\Program Files\Java\jdk-22)
SPARK_HOME = путь до папки с Apache Spark (в моем случае - C:\Program Files\Apache Spark)
```

Кроме того, следует создать 3 дополнительных системных переменных в окне системных переменных (нижнее окно), нажав на `Path` -> `Изменить` -> `Создать`:
```python
%HADOOP_HOME%\bin
%JAVA_HOME%\bin
%SPARK_HOME%\bin
```

Кроме того, следует перенести копию файла `hadoop.dll` в папку `C:\Windows\System32`, а также запустить командную строку (не обязательно от имени администратора) и прописать там команду:
```python
set PATH=%PATH%
```

## 5. Проверка работоспособности через Jupyter Notebook и командную строку
После всех манипуляций следует зайти в командую строку от имени администратора и проверить запуск Spark командой `spark-shell` и после этого запустить в браузере `localhost:4040`.
<p align="center">
  <img width="600" height="250" src="https://github.com/SvgPrizrak/Apache_Spark_Guide/blob/main/pictures/spark-shell.png">
</p>
<p align="center">
  <img width="600" height="220" src="https://github.com/SvgPrizrak/Apache_Spark_Guide/blob/main/pictures/localhost.png">
</p>

После проверки в командной строке зайти в `Jupyter Notebook` (если установлена Anaconda, то можно просто это сделать через поиск ОС), создать блокнот и запустить команды `!pip install pyspark` и `!pip install findspark` для установки пакетов (плюс так можно удостовериться в том, что Apache Spark будет работать правильно) - см. п. 6.

## 6. Конфликты Apache Spark и Apache Hadoop
При неверном выборе версии Apache Hadoop у меня возникала единственная проблема - я не мог сохранить какой-нибудь датафрейм в формате .csv и .xlsx. Для проверки этого следует запустить ноутбук `test_pyspark.ipynb` и `PySpark Training.ipynb`, который я приложу к данному гайду. Обычно конфликт возникает при выборе слишком новой версии Hadoop из 2 [репозитория](https://github.com/cdarlint/winutils) из п.3 гайда (с первым [репозиторием](https://github.com/steveloughran/winutils/tree/master таких проблем не наблюдалось)). Решение проблемы - взять вместо `hadoop-3.3.5/bin` другую версию по убыванию (`hadoop-3.2.2/bin`), полностью выполнить все процедуры из п.3 гайда с другой версией Hadoop (на Windows 11 поднялась версия 3.3.5, в то время как на Windows 10 поднялась только 3.2.2 на одной и той же версии Apache Spark).
